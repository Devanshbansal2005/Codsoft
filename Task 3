
import torch
import torchvision.transforms as transforms
import torch.nn as nn
import numpy as np
from torchvision.models import resnet50
from PIL import Image

# Load pre-trained ResNet-50 (Feature Extractor)
class ImageEncoder(nn.Module):
    def __init__(self):
        super(ImageEncoder, self).__init__()
        resnet = resnet50(pretrained=True)
        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1])  # Remove classification layer

    def forward(self, image):
        features = self.feature_extractor(image)
        return features.view(features.size(0), -1)

# Load Image and Extract Features
def extract_features(image_path, model):
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    image = Image.open(image_path).convert("RGB")
    image = transform(image).unsqueeze(0)
    with torch.no_grad():
        features = model(image)
    return features

# Simple LSTM Captioning Model
class CaptioningModel(nn.Module):
    def __init__(self, vocab_size, embed_size, hidden_size):
        super(CaptioningModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.lstm = nn.LSTM(embed_size, hidden_size)
        self.fc = nn.Linear(hidden_size, vocab_size)

    def forward(self, features, captions):
        captions = self.embedding(captions)
        lstm_out, _ = self.lstm(captions)
        outputs = self.fc(lstm_out)
        return outputs

# Dummy tokenizer (Replace with a trained one)
tokenizer = {"<start>": 1, "<end>": 2, "a": 3, "cat": 4}
vocab_size = len(tokenizer) + 1
max_length = 10

# Initialize Models
image_encoder = ImageEncoder().eval()
caption_model = CaptioningModel(vocab_size, 256, 512).eval()

# Generate Caption
def generate_caption(image_path):
    image_feature = extract_features(image_path, image_encoder)
    in_text = ["<start>"]
    for _ in range(max_length):
        sequence = torch.tensor([[tokenizer.get(w, 0) for w in in_text]], dtype=torch.long)
        with torch.no_grad():
            output = caption_model(image_feature, sequence)
        word = next((k for k, v in tokenizer.items() if v == output.argmax().item()), "<end>")
        in_text.append(word)
        if word == "<end>":
            break
    return " ".join(in_text[1:-1])

# Test with an image
print("Caption:", generate_caption("example.jpg"))
